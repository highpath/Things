# Batch Normalize & Dropout

## Batch Normalization
Deep Learning에서 Layer가 많아질 때 학습이 어려워지는 이유는 weight의 미세한 변화들이 누적되어 쌓이게 되면, Hidden Layer가 높아질수록 값의 변화의 폭이 커지기 때문입니다.

즉, Network의 각 층이나 Activation 마다 input의 distribution이 달라지는 현상이 Internal Covariate Shift 입니다.

Internal Covariate Shift를 해결하기 위해서 Activation Function의 변화 (RELU 등), 초기 weight의 설정, 낮은 Learning rate 등을 이용하였지만, 이는 근본적인 해결책이 아니며 Training 과정 자체를 전체적으로 안정화시킬 수 있는 방법이 필요합니다.

weight에 따른 weighted sum의 변화의 폭이 줄어든다면 학습이 잘될 것이라는 Batch Normalization의 기본 가정입니다.

Batch Normalization은 간단하게 각 층의 input의 distribution을 평균 0, 표준 편차 1인 distrivution으로 Normalize 시켜줍니다.

하지만, 단순히 distribution을 평균 0, 표준 편차 1인 distribution으로 Normalize시키는 방법은 Activation Function의 non-linearity를 없앨 수 있기 때문에 normalize 된 값에 scale factor가 추가되고, 이 factor는 Back-propagation 과정에서 Train 됩니다.

따라서, Batch Normalization을 이용하면 Internal Covariate Shift를 제거할 수 있기 때문에 빠른 학습(보다 큰 Learning Rate 사용 가능)이 가능하고, weight의 initialize를 크게 고려하지 않고도 vanishing/exploding gradient 문제를 막을 수 있게 됩니다.

실제로 Batch Normalization을 Network에 적용시킬 때에는 특정 Hidden Layer에 들어 가기 전에 Batch Normalization Layer를 추가하여 input의 distribution을 바꾼 뒤 Activation Function으로 넣어주는 방식으로 이용됩니다.

## Dropout

일반적으로 신경망에서 Hidden Layer의 개수가 많아지면 학습 능력이 좋아지는 반면, overfitting에 빠질 가능성이 높고 학습 시간이 길어지는 문제점이 있습니다.

학습의 성능을 개선하기 위한 방법 중 하나는 모델 결합(Model Combination)입니다.

서로 구조가 다른 Network를 학습하여 그 결과를 결합하는 방식인데, 이는 여러 명의 전문가 집단의 각각의 의견을 조합하여 하나의 결과를 도출해내는 과정에 비유할 수 있습니다.

하지만, 깊은 망을 가진 1개의 Network를 학습하는 것도 엄청난 시간이 소요되는데 복수 개의 망을 학습시키는 것은 매우 어려운 일입니다.

또한, 복수 개의 망을 학습시켰을 때에도 실행 시 학습된 망을 모두 실행시켜야 하기 때문에 연산 시간 역시 늘어나게 됩니다.

Dropout은 Network의 일부를 생략하는 것입니다.

생략된 일부의 Network는 학습에 영향을 끼치지 않게 됩니다.

여러 개의 모델을 각각 Training 하는 과정 없이, 무작위로 일부 Network를 생략하여 다른 Model을 학습한 것과 같은 효과를 얻을 수 있습니다.

또한, 학습을 진행하다 보면, 각각의 Weight들이 서로 동조화(co-adaptation)되는 현상이 발생할 수 있습니다.

학습 시 cost(loss) function을 줄여나가는 과정에서 특정 hidden unit의 결과가 다른 hidden unit에 영향을 주기 때문에 결과적으로 각각의 다른 hidden unit들이 비슷한 출력을 가지게 됩니다.

하지만, Dropout을 이용하게 되면 hidden unit의 활성도가 조절되어, 다른 hidden unit의 결과에 영향을 줄여주기 때문에 각각의 hidden unit은 독립적으로 feature를 얻을 수 있게 되어 좀 더 선명하고, 서로 간의 correlation이 낮은 feature를 얻을 수 있게 됩니다.

Dropout을 실제로 Training 시킬 때는 일부 Network가 생략된 모델을 따로 실행시키는 것이 아니라, 생략된 모델이 모두 파라미터를 공유하고 있기 때문에 각각의 Neuron들이 존속할 확률을 각각의 가중치에 곱해주는 형태가 됩니다.

또한, 실행 시에는 일부 Network를 생략하지 않고 각각의 Weight에 존속할 확률을 곱해주는 형태가 됩니다.
