# 학습속도 저하 & Overfitting

## Neural Network에 반드시 필요한 부분 - 학습
Neural Network에서는 Inputs을 나름대로 취합하여 최종출력과 비슷한 결과를 낼 수 있도록 계속해서 Weight를 조절해주는 과정이 필요하며, 이를 학습이라 지칭합니다.

다르게 생각하면 Neural Network에서 Weight를 조절해주는 과정, 즉 학습과정은 반드시 필요한 부분이 됩니다.

이 학습 과정에서 Cost Function과 Activation Function 개념이 중요하게 사용됩니다.


### Cost Function이란?
Cost Function은 weight와 bias로 이루어져 있으며, Weight 값을 조절하여 값이 작아질수록 학습이 잘 된다고 볼 수 있습니다.

일반적으로 MES(mean square error)를 많이 사용합니다.

### Activation Function이란?
어떠한 신호를 입력받아 이를 적절한 처리를 하여 출력해주는 함수입니다.

가장 많이 사용하는 함수는 Sigmoid Function으로 0과 1 사이의 값만 가질 수 있도록 하는 비선형 함수입니다.

이 함수는 미분이 가능하도록 0과 1 사이를 부드럽게 이어줍니다.


시그모이드 함수

(e^x)/(e^x + 1)


## 학습 속도의 저하 - 원인
먼저 일반적으로 신경망에 사용하는 Cost Function은 MES, Activation Function은 Sigmoid 함수를 사용합니다.

그러나 Sigmoid 함수의 미분 특성으로 인해 결합하게 되면서 학습이 느려지는 현상이 발생하게 됩니다.

예를 들어 1개의 뉴런이 있고, Sigmoid 함수를 사용한다면 입력 x를 가하면 z=wx+b 가 되고 이것이 활성함수(Sigmoid Function)를 거치면 출력 a가 나오게 되고, 만약 입력 x에 대한 기대값을 y라고 한다면 cost functioon은 다음과 같습니다.

C = {(y-a)^2}/2

(y-a)는 오차가 되고, 에러를 back-propagation하면 오차가 클수록 속도는 빨라져야 되는데 그렇지 못합니다.

가중치(w)와 바이어스(b)값을 갱신하려면 cost function을 가중치와 바이어스에 대해서 편미분을 수행하고 Sigmoid 함수의 미분값을 곱하는 부분이 있어 학습속도 저하의 원인이 됩니다.

Sigmoid 함수의 미분값은 0일 때 최대값을 갖고, 0으로부터 멀어질수록 0에 수렴하는 작은 값으로 나오게 되어 바이어스나 가중치의 갱신값에 아주 작은 값을 곱해주는 형태가 되기 때문에 (y-a)가 아무리 크더라도 아주 작은 값이 되기 때문에 학습 저하가 일어나게 됩니다.

## 속도 개선 - ReLU(Rectified Linear Unit) function
Sigmoid function의 문제점을 해결하기 위해 생겨난 활성화 함수로 가장 큰 장점은 미분이 아주 간단하게 된다는 것입니다.

f = (x<0) f(x) =0, (x>=0) f(x) = x

## 학습 능력의 저하 - Overfitting
Overfitting이란 학습을 할 때 한정된 데이터에 너무 특화가 되어 새로운 데이터에 대한 결과가 나빠지거나 학습효과가 나타나지 않는 경우를 나타냅니다.

## 학습 능력의 개선 - Regularaization, DropOut
보통 학습은 Cost function이 작아지는 쪽으로 진행을 하게 되는데 단순하게 작아지는 쪽으로만 진행을 하다 보면, 특정 가중치 값들이 커지면서 오히려 결과를 나쁘게 하는 경우가 생기게 됩니다.

Regularization은 penalty라는 개념을 도입하여 복잡한 쪽보다는 간단한 쪽으로 선택을 유도하는 방식입니다.

즉, 특정 가중치 값들이 작아지도록 학습을 진행하게 되고 이는 일반화에 적합한 특성을 갖게 만드는 것이라고 볼 수 있습니다.

DropOut은 망 자체를 변화시키는 방법입니다.

Hidden Layer의 수가 많아질 경우 Deep Neural Network가 되면서 문제해결 가능성이 높아지지만 학습 시간이 길어지고, Overfitting에 빠질 가능성이 높아집니다.

DropOut은 망 내의 모든 layer에 대해 학습을 수행하는 것이 아니라 일부 뉴런을 생략하고 줄어든 신경망을 통해 학습을 수행합니다.

또 추가적으로 기존 학습데이터를 변형시키면서 학습데이터 양을 증가시키는 방법이 있습니다.