# CNN 구조 3 - VGGNet, ResNet

## VGGNet
2014년 ILSVRC에서 GoogleNet과 함께 주목을 받았고 간소한 차이로 2위를 차지했습니다.

하지만 VGGNet은 간단한 구조와 단일 network에서 좋은 성능 등을 보여준다는 이유로, 오히려 1위를 차지한 GoogleNet보다 많은 network에서 응용되고 있습니다.

VGGNet은 사실 network의 깊이(depth)가 어떤 영향을 주는지 연구를 하기 위해서 설계된 network입니다.

그래서인지 convolution kernel 사이즈를 한 사이즈로 정하고 Convolution의 개수를 늘리는 방식으로 테스트를 진행합니다.

Convolution layer는 kernel 사이즈 3x3, padding 사이즈 1로 설정해서 convolution layer를 통해서 이미지 resize를 한 것이 아니라 max pooling을 사용해서 이미지 resize를 합니다.

Max pooling은 kernel 사이즈 2x2, stride 2로 이미지를 절반으로 resize 합니다.

아래 그림은 VGGNet이 layer의 깊이를 어떻게 늘려가며 테스트했는지 보여줍니다.

Network는 convolution layer를 추가함으로써 깊이가 깊어지는 걸 보실 수 있습니다.

이론적으로 보면 5x5 kernel이나 3x3 kernel을 2개 사용하는 거나 성능적으로는 동일할 것입니다.

하지만 VGGNet 팀이 테스트한 결과 3x3 kernel을 중첩해서 사용하는게 성능은 더 좋다고 합니다.

그 이유는 non-linear 함수 ReLU가 더 많이 들어가서, decision function이 더 잘 학습된다고 볼 수 있습니다.

또 한가지 이유는 학습을 해야 할 parameter 수가 줄어듭니다.

그러므로 큰 사이즈의 kernel 보다는 작은 사이즈 kernel을 중첩해서 사용하는 게 성능도 더 좋고, 학습할 parameter도 적어서 속도도 더 빨라집니다.

VGGNet의 구조는 간단하지만 최종단에 fully-connected layer 3개를 붙여서 parameter 수가 너무 많다는 단점이 있습니다.

최종단 fully-connected layer에만 parameter 개수가 약 122 million개나 됩니다.

위에 그림은 VGGNet의 깊이에 따른 오차를 계산한 테이블입니다.

보시는 바와 같이 network의 깊이가 깊어질수록 error는 내려가는 걸 보실 수 있습니다.

하지만 D(16-layer)와 E(19-layer)를 비교하면 error가 비슷하거나 더 나빠지는 걸 보실 수 있습니다.

그렇다면 19-layer보다 network를 더 깊게 설계하면 어떻게 될까요?

이 테이블을 보면 결과는 더 나빠질 거라고 추측할 수 있습니다.

## ResNet

VGGNet에서도 언급했지만, deep learning은 기본적으로 망이 깊어지면 성능이 더 좋아진다고 생각을 합니다.

이전까지 설계된 deep learning network는 8-layer 수준이였고, VGGNet에서 19-layer까지 테스트를 하였습니다.

하지만 VGGNet의 16-layer의 성능 차이는 거의 없었습니다.

만약 19-layer보다 더 망을 깊게 설계하면 어떻게 될까요?

### Deep network의 문제점

RsNet 팀이 망이 깊어지는 경우 어떤 결과가 나오는지 실험을 하였습니다.

