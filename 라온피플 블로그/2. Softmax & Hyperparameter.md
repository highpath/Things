# Softmax & Hyperparameter

## Activation Function

## Softmax Function
Multiclass Classification 문제에서는 Softmax function이 대표적인 Activation function으로 쓰인다.


Multiclass Classification : the problem of classifying instances into one of three or more classes

특징 : 클래스의 결과 값은 0~1로 정규화되며, 모든 클래스의 결과 값의 합은 1이다.
따라서 클래스의 결과 값은 각 클래스에 해당할 확률과 같다.

## Hyperparameter
Deep Learning은 weight 값을 스스로 학습하여 최적의 신경망을 스스로 찾아내지만, 실제로 학습 시킬 때에는 사람이 직접 튜닝해 주어야 할 다양한 변수가 있습니다.

1. Learning rate
Learning rate는 결과의 오차를 학습에 얼마나 반영할 지를 결정하는 변수입니다.

2. Cost function
Cost function은 입력에 따른 기대 값과 실제 값의 차이를 계산하는 함수입니다.

- Mean Squre Error (평균 제곱 오차)
- Cross-Entropy Error (교차 엔트로피 오차)

3. Regularization parameter
Overfitting의 문제를 피하기 위해 L1, L2 regularization 방법을 사용할 수 있습니다.


-
Regularization basically adds the penalty as model complexity increases.
Regularization parameter (lambda) penalizes all the parameters except intercept so that model generalizes the data and won't overfit.

In order to create less complex (parsimonious) model when you have a large number of features in your dataset, some of the Regularization techniques used to address 

A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.

The key difference between these two is the penalty term.

Ridge regression adds "squared magnitude" of coefficient as penalty term to the loss function.

Here, if lambda is zero then you can imagine we get back OLS

OLS : Ordinary least squares is a type of linear least squares method for estimating the unknown parameters in a linear regression model.

least squares : standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) ...

However, if lambda is very large then it will add too much weight and it will lead to under-fitting.

Having said that it's important how lambda is chosen.

This technique works very well to avoid over-fitting issue.

Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds "absolute value of magnitude" of coefficient as penalty term to the loss function

