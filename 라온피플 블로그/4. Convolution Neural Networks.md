# Convolution Neural Networks (CNN)

## CNN 개념이 나오기 전

기존 Fully-connected Multi-layered Neural Network는 이미지의 특성을 이해하지 못하고 이미지를 단순 1D 데이터로 보고 학습을 합니다.

하지만 이미지는 2D이며, 이미지가 형성될 때 가장 영향을 많이 끼치는 것은 인접한 pixel입니다.

그리고 이미지를 구분할 때 중요한 pixel이 있고, 필요 없는 pixel도 있을 겁니다.

그래서 고안된 방식이 CNN 입니다.


## Convolution 이란?

현재 위치의 출력 데이터는 인접한 Pixel에 Convolution Filter를 곱해서 얻어진 값입니다.

즉, 인접한 Pixel 값의 영향력이 출력 데이터라고 보시면 됩니다.

그럼 영향력은 어떻게 정할까요?

그 영향력은 Convolution Filter가 결정합니다.
그래서 이미지 처리 분야에서 특정 Feature들을 추출하기 위해 Convolution을 사용합니다.

아래는 Sobel Filter라고 이미지 처리분야에서 유명한 Convolution Filter입니다.

왼쪽의 입력 데이터가 가운데와 같은 Convolution Filter를 거치면 오른쪽 출력 데이터와 같은 Feature Map이 생성됩니다.

Sobel Filter는 윤곽선을 Feature로 하는 Convolution 연산이라고 보시면 됩니다.

전통적인 이미지 처리에서는 사용자가 직접 추출하고자 하는 Feature Map에 따라 Convolution Filter의 값을 설정해주었습니다.

하지만 Deep Learning 에서 Convolution은 컴퓨터가 학습을 통해서 Convolution Filter의 값을 설정합니다.

그래서 학습을 통해 불변하는 Feature를 출력 데이터로 나올 수 있게 Convolution Filter의 값을 학습합니다.

## Pooling 이란?

Pooling은 Convolution과 비슷하게 인접한 Pixel 값만을 사용하지만, 특별히 곱하거나 더하는 연산은 없습니다.

대표적인 Pooling 방법은 Max Pooling과 Average Pooling입니다.

Max Pooling은 인접한 Pixel에서 가장 큰 값을 새로운 Pixel 값으로 정하고, Average Pooling은 인접한 Pixel의 평균 값을 새로운 Pixel 값으로 정합니다.

Pooling Layer는 아래와 같은 특징이 있습니다.

- 학습해야 할 매개 변수가 없습니다.
- 채널 수가 변하지 않습니다.
- 입력의 변화에 영향을 적게 받아 입력 데이터가 조금 변해도 Pooling의 결과는 잘 변하지 않습니다.

## Im2col 이란?
CNN은 1989년에 고안되었지만, 범용화를 하기에는 많이 부족했습니다.

그 이유는 Convolution은 특정 Size의 Window를 입력 이미지의 각 pixel 마다 Size 제곱의 개수만큼 곱셈 연산을 수행합니다.

그리고 1개의 Feature Map만 연산하는 것이 아니라 많은 Feature Map을 연산합니다.

그러므로 학습해야 할 parameter는 적지만 연산 시간은 많이 걸리게 됩니다.

그래서 보다 빠르게 Convolution을 연산하기 위해 이미지 데이터를 Convolution하기 좋은 연속된 데이터로 변환해서 Convolution을 합니다.

이런 변환을 Im2col이라고 합니다.

Im2col 변환을 사용하면 빠른 Convolution이 가능합니다.

Convolution 연산을 하기 위해 필요한 pixel 값들을 그림처럼 펼칩니다.

만약 3x3 Window를 사용해서 Convolution을 수행한다고 하면 Im2col 변환을 하면 입력 데이터가 9배 커지게 됩니다.

그래서 Im2col 변환은 Im2col 변환은 Convolution 연산은 빠르지만 연산 메모리가 증가하는 단점이 있습니다.

## 

Classification 분야의 CNN의 과정은 크게 보면 다음과 같은 3단계 과정으로 이루어져 있습니다.

1. 특징을 추출하기 위한 단계 (Convolution Layer)
2. Topology 변화에 영향을 받지 않도록 하는 단계 (Sub-Sampling, Pooling Layer)
3. 분류기 단계

Input - FEature extraction - Shift and distortion invariance - Classification - output

topology : the way in which constituent parts are interrelated or arranged
위상수학(Topology) : 

## CNN 연산에의 Parameter

Convolution Filter 개수, Filter Window Size, Padding 여부, Stride

1. Convolution Filter
Convolution Filter 개수는 각 Layer에서의 연산 시간/량을 비교적 일정하게 유지하여 시스템의 균형 맞추는 것이 좋습니다.

보통 Sub-Sampling을 거치면 1/4로 출력이 줄어 들기 때문에 Feature Map의 개수는 대략 4배 정도로 증가시키면 됩니다.

Sub-sampling techniques have recently been frequently used for increasing the accuracy of a classifier.

The basic idea is to train classifiers on multiple subsamples of the data and combine their predictions, usually by voting.

Subsampling reduces the image size by removing information all together.

Usually when you subsample, you also interpolate or smooth the image so that you reduce aliasing.

2. Filter Window Size

작은 필터를 여러 개 중첩하면 중간 단계에 있는 Non-Linearity를 활용하여 원하는 특징을 좀 더 돋보이도록 할 수 있습니다.

뿐만 아니라 연산량도 더 적습니다.

그래서 요즘 대부분의 CNN은 3x3 Filter Window Size를 중첩해서 사용합니다.

3. Padding

Padding은 Convolution을 수행하기 전, 입력 데이터 주변을 특정 pixel 값으로 채워 늘리는 것을 말합니다.

Padding을 사용하는 이유는 Padding을 사용하지 않을 경우, 출력 데이터의 크기는 Convolution Layer를 지날 때마다 작아지게 되고, 가장자리의 정보들이 사라지는 문제가 발생할 수 있습니다.

보통은 Padding을 수행하고, Sub-Sampling으로 데이터 Resize를 진행합니다.

4. Stride
Stride는 Window의 이동할 간격을 조절하는 Parameter입니다.

Stride 값이 커지면, 결과 데이터 사이즈가 작아지게 됩니다.

이번 Chapter를 요약하자면, CNN은 Convolution과 Sub-Sampling을 반복적으로 거치면서 불변하는 특징을 찾고, 그 특징을 입력 데이터로 Fully-connected Multi-layered Neural Network로 Classification을 수행합니다.