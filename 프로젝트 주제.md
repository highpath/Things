# 프로젝트 주제

## Jigsaw Unintended Bias in Toxicity Classification
Detect toxicity across a diverse range of conversations

### Description

Can you help detect toxic comments - and minimize unintended model bias?

That's your challenge in this competition


The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation.

A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.

Last year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity.

This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.

Here's the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity.

Models predicted a high likelihood of toxicity for comments containing those identities (e.g. "gay"), even when those comments were not actually toxic (such as "I am a gay woman").

This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways.

Training a model from data with these imbalances risks simply mirroring thos biases back to users.


In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities.

You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias.

Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.

## Tweet Sentiment Extraction

### Description

"My ridiculous dog is amazing." [sentiment: positive]

With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), 

## Real or Not? NLP with Disaster Tweets
Predict which Tweets are about real disasters and which ones are not

### Description

This particlar challenge is perfect for data scientists looking to get started with Natural Language Processing.

1. 무친 말 찾아내기
a year ago
3165 teams

2. 텐서플로 질문 답변 하는거 구별하기
5 months ago
1233 teams

3. 자동차 보험을 내년에도 들건지 예측
3 years ago
5136 teams

4. 트위트에서 말 보고 감정 추출하기
17 days ago
2227 teams

5. 재난 트윗이 진짜인지 찾아내기 X
Ongoing
1491 teams

# Kaggle Lingo

## Kaggle
- Home of data science

Code + data + forums + competitions + courses + you!

- Made-up word

- Rhymes with "gaggle" (a group of geese)

## Kernels
- Hosted coding environment
- Runs in the browser (no downloading)
- Write, run & share data science code

Types:
- Notebooks: hosted Jupyter notebooks (text + code)
- Scripts: Raw Python, R or R Markdown files (code only)

## More Kernels Stuff

- Cell: in notebooks, cells are editable blocks of text or code

- Docker: A docker is basically a whole file system. We have two (one in Python, one in R) with everything your need to run code written on Kaggle locally. You can download them from the Kaggle Github.

- Commit: Commiting code runs it from top to bottom & saves a static version you can refer back to. (Like a save file in a video game.)

## Competitions

- Featured: Them most competitive competition with the biggest prizes. Often hosted by corporations looking to improve their current approach.

Only featured(and some research) competitions award points. Points are used to determine overall user rankings.

- Analytics: Not supervised machine learning. Many of these are designed to help nonprofits with tricky problems they're facing.

- Research: More theoretical problems based on active areas of research (academic & industrial).

- Getting Started: Designed to be your first or second

- Playground: Especially interesting problems we think you might enjoy trying to sovle or fun seasonal competitions.

- InClass: Lets you host your own Kaggle competition. (They don't have to be for an actual class, that's just why is was started)

- Leaderboard: The place where all the scores for entries to a specific competition are shown.
    - Public: This is the one that you see during the competition.
    - Private: You only see this after the competition ends. Usually scored on a different part of the data.

- Shakeup: Changes in ranking when the private leaderboard is shown at the end of a competition. (Usually people move down the ranks due to overfitting.)

- In the money: people at that rank will win money. (If they still have that rank on the private leaderboard after the shakeup.)

## Leakage
- When information about the target variable is accidentally included in the training data
    - All photos of the target class were taken with a specific camera, so you end up actually building a camera type detector
    - You're trying to predict if someone will get an illness, but you don't remove information about whether someone was treated for that illness.

Out of >3 Million Kagglers there are:
- Competitions: 145 Grandmasters, 1220 Masters
- Kernels: 11 Grandmasters, 49 Masters
- Discussion: 12 Grandmasters, 20 Masters

It's pretty hard to become a Master or Grandmaster, but you can get there with hard work & dedication!


- Kaggle data scientists work very hard to minimize leakage in competitions

## Kaggle Progression System

Novice -> Contributor -> Expert -> Master -> Grandmaster

- Once you reach Contributor, your votes count for awarding other people medals!

- Once you reach Expert, you get ranked separately in three categories: discussion, kernels and competitions

- Medals: These are awarded for having your content upvoted or being near/at the top of the leaderboard for competitions.

# How to Enter a Kaggle Competition (using Kernels)

## 
step

1. you need to train your model

